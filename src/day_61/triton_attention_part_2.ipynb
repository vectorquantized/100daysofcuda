{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f\"cuda:{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def attention_kernel(Q_ptr: torch.Tensor, \n",
    "                     K_ptr: torch.Tensor, \n",
    "                     V_ptr: torch.Tensor, \n",
    "                     output_ptr: torch.Tensor,\n",
    "                     batch_stride: int, seq_stride: int,\n",
    "                     head_stride: int,\n",
    "                     B: int, L: int, heads: int, d_k: int,\n",
    "                     MAX_D_K: tl.constexpr, \n",
    "                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    \n",
    "    pid_BH = tl.program_id(0)\n",
    "    batch_idx = pid_BH // heads\n",
    "    head_idx = pid_BH % heads\n",
    "    \n",
    "    query_block_idx = tl.program_id(1)\n",
    "    query_start = query_block_idx * BLOCK_SIZE\n",
    "    \n",
    "    num_blocks = tl.cdiv(d_k, BLOCK_SIZE)\n",
    "    offs_row = tl.arange(0, BLOCK_SIZE)\n",
    "    offs_col = tl.arange(0, MAX_D_K)\n",
    "    q_block_ptr = Q_ptr + batch_idx * batch_stride + head_idx * head_stride + query_start * seq_stride\n",
    "    k_base_ptr = K_ptr + batch_idx * batch_stride + head_idx * head_stride\n",
    "    v_base_ptr = V_ptr + batch_idx * batch_stride + head_idx * head_stride\n",
    "    Q_start = q_block_ptr + offs_row[:, None] * MAX_D_K + offs_col[None, :]\n",
    "    \n",
    "    q = tl.load(Q_start, \n",
    "                mask=(query_start + tl.arange(0, BLOCK_SIZE)[:, None] < L) & (offs_col[None, :] < d_k), \n",
    "                other=0.0) # (BLOCK_SIZE, d_k)\n",
    "    \n",
    "    \n",
    "    m_i = tl.full((BLOCK_SIZE,), -float('inf'), tl.float32)\n",
    "    l_i = tl.zeros((BLOCK_SIZE,), tl.float32)\n",
    "    acc = tl.zeros((BLOCK_SIZE, d_k), tl.float32)\n",
    "    for key_block in tl.range(0, L, BLOCK_SIZE, num_stages=num_stages):\n",
    "        mask = (offs_row  + k * BLOCK_SIZE < d_k)[None, :]\n",
    "        K_tile_ptrs = k_base_ptr + key_block * seq_stride\n",
    "        V_tile_ptrs = v_base_ptr + key_block * seq_stride\n",
    "        mask = (key_block + offs_row[None, :]) < L & (offs_col[:, None] < d_k)\n",
    "        k_t = tl.load(K_tile_ptrs + offs_row[None, :] * MAX_D_K + offs_col[:, None], \n",
    "                      mask=k_mask, other=0.0) # (d_k, BLOCK_SIZE)\n",
    "        v = tl.load(V_tile_ptrs + offs_row[:, None] * d_k + offs_col[None, :], \n",
    "                   mask=mask, other=0.0) # (BLOCK_SIZE, d_k)\n",
    "        scores = tl.dot(q, k_t)\n",
    "    \n",
    "        scores *= tl.rsqrt(d_k * 1.0) # (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    \n",
    "        m_new = tl.maximum(m_i, tl.max(scores, axis=1)) # (BLOCK_SIZE)\n",
    "        exp_scores = tl.exp(scores - m_new[:, None]) # (BLOCK_SIZE, BLOCK_SIZE)\n",
    "        l_new = l_i * tl.exp(m_i - m_new) + tl.sum(exp_scores, axis=1) # (BLOCK_SIZE)\n",
    "    \n",
    "        acc = acc *  tl.exp(m_i - m_new) [:, None] + tl.dot(exp_scores, v) # (BLOCK_SIZE, d_k) \n",
    "        m_i = m_new\n",
    "        l_i = l_new \n",
    "    \n",
    "    \n",
    "    out = acc / l_i[:, None]\n",
    "    out_base_idx = batch_idx * batch_stride + head_idx * head_stride + query_start * seq_stride\n",
    "    out_ptrs = out_ptr + out_base_idx + offs_row[:, None] * d_k + offs_col[None, :]\n",
    "    out_mask = offs_row[:, None] < (L - query_start)\n",
    "    tl.store(out_ptrs, out, mask=out_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tl.transpose` not found.\n"
     ]
    }
   ],
   "source": [
    "tl.transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_triton(q, k, v):\n",
    "    \"\"\"\n",
    "    Compute attention using Triton kernel\n",
    "    Args:\n",
    "        q: (batch_size, num_heads, seq_len, d_k)\n",
    "        k: (batch_size, num_heads, seq_len, d_k)\n",
    "        v: (batch_size, num_heads, seq_len, d_k)\n",
    "    Returns:\n",
    "        output: (batch_size, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, d_k = q.shape\n",
    "    \n",
    "    # Output tensor\n",
    "    output = torch.empty_like(q)\n",
    "    \n",
    "    # Calculate strides\n",
    "    batch_stride = num_heads * seq_len * d_k\n",
    "    head_stride = seq_len * d_k\n",
    "    seq_stride = d_k\n",
    "    \n",
    "    # Block size configuration\n",
    "    BLOCK_SIZE = 32  # Adjust based on your GPU architecture\n",
    "    num_stages = 3\n",
    "    \n",
    "    # Grid configuration\n",
    "    grid = (batch_size * num_heads, triton.cdiv(L, BLOCK_SIZE))\n",
    "    \n",
    "    # Launch kernel\n",
    "    attention_kernel[grid](\n",
    "        q, k, v, output,\n",
    "        batch_stride, seq_stride, head_stride,\n",
    "        batch_size, seq_len, num_heads, d_k,\n",
    "        BLOCK_SIZE, num_stages\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 19:15:\n                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n\n    pid_BH = tl.program_id(0)\n    batch_idx = pid_BH // heads\n    head_idx = pid_BH % heads\n\n    query_block_idx = tl.program_id(1)\n    query_start = query_block_idx * BLOCK_SIZE\n\n    num_blocks = tl.cdiv(d_k, BLOCK_SIZE)\n    offs_row = tl.arange(0, BLOCK_SIZE)\n    offs_col = tl.arange(0, d_k)\n               ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/core.py:35\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_builder` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/core.py:1192\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, end, _builder)\u001b[0m\n\u001b[1;32m   1191\u001b[0m end \u001b[38;5;241m=\u001b[39m _constexpr_to_value(end)\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msemantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/semantic.py:503\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, end, builder)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(end, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marange\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms arguments must be of type tl.constexpr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    504\u001b[0m is_start_int64 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(start \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: arange's arguments must be of type tl.constexpr",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, num_heads, seq_len, d_k, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute attention using Triton kernel\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m output_triton \u001b[38;5;241m=\u001b[39m \u001b[43mattention_triton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m, in \u001b[0;36mattention_triton\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m     26\u001b[0m grid \u001b[38;5;241m=\u001b[39m (batch_size \u001b[38;5;241m*\u001b[39m num_heads, triton\u001b[38;5;241m.\u001b[39mcdiv(d_k, BLOCK_SIZE))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Launch kernel\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mattention_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:276\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    278\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 19:15:\n                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n\n    pid_BH = tl.program_id(0)\n    batch_idx = pid_BH // heads\n    head_idx = pid_BH % heads\n\n    query_block_idx = tl.program_id(1)\n    query_start = query_block_idx * BLOCK_SIZE\n\n    num_blocks = tl.cdiv(d_k, BLOCK_SIZE)\n    offs_row = tl.arange(0, BLOCK_SIZE)\n    offs_col = tl.arange(0, d_k)\n               ^"
     ]
    }
   ],
   "source": [
    "# Test parameters\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "seq_len = 16\n",
    "d_k = 64\n",
    "\n",
    "# Generate random data\n",
    "torch.manual_seed(0)  # For reproducibility\n",
    "q = torch.randn(batch_size, num_heads, seq_len, d_k, device=\"cuda\", dtype=torch.float32)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, d_k, device=\"cuda\", dtype=torch.float32)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, d_k, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# Compute attention using Triton kernel\n",
    "output_triton = attention_triton(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_builder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Computes the element-wise inverse square root of :code:`x`.\n",
      "\n",
      ":param x: the input values\n",
      ":type x: Block\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;34m@\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0m_check_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fp32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fp64\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0m_add_math_1arg_docstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inverse square root\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_member_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_builder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_builder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_rsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/math.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "tl.rsqrt??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
