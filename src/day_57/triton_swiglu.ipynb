{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"properties={'max_shared_mem': 101376, 'max_num_regs': 65536, \"\n",
      " \"'multiprocessor_count': 64, 'warpSize': 32, 'sm_clock_rate': 1695000, \"\n",
      " \"'mem_clock_rate': 8001000, 'mem_bus_width': 384}\")\n"
     ]
    }
   ],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "pprint(f\"{properties=}\", underscore_numbers=True)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _swiglu_forward_kernel(input_ptr: torch.Tensor, up_ptr: torch.Tensor, gate_ptr:torch.Tensor,\n",
    "                         output_ptr: torch.Tensor, input_batch_stride: int, input_seq_stride: int, \n",
    "                         output_batch_stride: int, output_seq_stride: int,\n",
    "                         L: int, H: int, O:int, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_O: tl.constexpr):\n",
    "    '''\n",
    "    Triton kernel for gelu.\n",
    "    define silu(x) = x * sigmoid(x)\n",
    "    gelu(x, up, gate) = up(x) * silu(gate(x))\n",
    "    x \\in R^{B x L x H}\n",
    "    up \\in R^{O x H} \\implies up(x) \\in R^{B x L x O}\n",
    "    gate \\in R^{O x H} \\implies gate(x) \\in R^{B x L x O}\n",
    "    gelu \\in R^{B x L x O}\n",
    "    Args:\n",
    "        input_ptr: Pointer to the input, shape: (B, L, H)\n",
    "        up_ptr: Pointer to the weights of Linear layer, shape: (O, H)\n",
    "        gate_ptr: Pointer to the weights of the Linear layer, shaoe: (O, H)\n",
    "        output_ptr: Pointer to output, shape: (B, L, O)\n",
    "        input_batch_stride: number of elements we move to reach next batch in the input\n",
    "        input_seq_stride: number of elements we move to reach next sequence in the input\n",
    "        output_batch_stride: number of elements we move to reach next batch in the output\n",
    "        output_seq_stride: number of elements we move to reach next sequence in the output\n",
    "        L: Sequence Length\n",
    "        H: Embedding Dimension\n",
    "        O: Output Dimension\n",
    "    '''\n",
    "    pid_BL = tl.program_id(0)\n",
    "    batch_idx = pid_BL // L\n",
    "    seq_idx = pid_BL % L\n",
    "    input_base_idx = batch_idx * input_batch_stride + seq_idx * input_seq_stride \n",
    "    output_base_idx = batch_idx * output_batch_stride + seq_idx * output_seq_stride\n",
    "    input_start_ptr = input_ptr + input_base_idx\n",
    "    output_start_ptr = output_ptr + output_base_idx\n",
    "    pid = tl.program_id(1)\n",
    "    num_pid_h = tl.cdiv(H, BLOCK_SIZE_H)\n",
    "    num_pid_o = tl.cdiv(O, BLOCK_SIZE_O)\n",
    "    pid_h = pid // num_pid_o\n",
    "    pid_o = pid % num_pid_o\n",
    "    offs_ah = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n",
    "    offs_bo = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n",
    "    offs_h = tl.arange(0, BLOCK_SIZE_H)\n",
    "    a_ptrs = input_start_ptr + offs_ah # shape (BLOCK_SIZE_H)\n",
    "    b_ptrs = gate_ptr + offs_bo[:, None] * H  + offs_h[None, :] # shape: (BLOCK_SIZE_O, BLOCK_SIZE_H)\n",
    "    c_ptrs = up_ptr + offs_bo[:, None] * H + offs_h[None, :]\n",
    "    gated = tl.zeros((BLOCK_SIZE_O,), dtype=tl.float32)\n",
    "    up = tl.zeros((BLOCK_SIZE_O, ), dtype=tl.float32)\n",
    "    for k in tl.range(0, tl.cdiv(H, BLOCK_SIZE_H)):\n",
    "        offs_h = tl.arange(0, BLOCK_SIZE_H)\n",
    "        a_tile_ptrs = a_ptrs + k * BLOCK_SIZE_H\n",
    "        mask_a = (offs_ah + k * BLOCK_SIZE_H < H)\n",
    "        b_tile_ptrs = b_ptrs + k * BLOCK_SIZE_H\n",
    "        mask_b = (offs_h + k * BLOCK_SIZE_H < H)[None, :]\n",
    "        c_tile_ptrs = c_ptrs + k * BLOCK_SIZE_H\n",
    "        a = tl.load(a_tile_ptrs, mask=mask_a, other=0.0)\n",
    "        b = tl.load(b_tile_ptrs, mask=mask_b, other=0.0)\n",
    "        c = tl.load(c_tile_ptrs, mask=mask_b, other=0.0)\n",
    "        gated += tl.sum(b * a, axis=1)\n",
    "        up += tl.sum(c * a, axis=1)\n",
    "    \n",
    "    \n",
    "    silu = gated * tl.sigmoid(gated)\n",
    "    output = up * silu\n",
    "    output_ptrs = output_start_ptr + offs_bo\n",
    "    mask_output = offs_bo < O\n",
    "    tl.store(output_ptrs, output, mask=mask_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swiglu(x: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, H = x.shape\n",
    "    O = 4 * H\n",
    "    \n",
    "    # Create weight matrices (these would normally be module parameters)\n",
    "    up_weights = torch.empty((O, H), dtype=torch.float32, device=x.device)\n",
    "    gate_weights = torch.empty((O, H), dtype=torch.float32, device=x.device)\n",
    "    \n",
    "    # Initialize weights (in a real implementation, these would be trained parameters)\n",
    "    # This is just placeholder initialization\n",
    "    torch.nn.init.kaiming_uniform_(up_weights)\n",
    "    torch.nn.init.kaiming_uniform_(gate_weights)\n",
    "    \n",
    "    num_stages = 8\n",
    "    BLOCK_SIZE_H = 256\n",
    "    BLOCK_SIZE_O = 512\n",
    "    grid = lambda META: (B * L, triton.cdiv(H, META['BLOCK_SIZE_H']) * triton.cdiv(O, META['BLOCK_SIZE_O']), )\n",
    "    output = torch.empty((B, L, O), dtype=torch.float32, device=x.device)\n",
    "    _swiglu_forward_kernel[grid](x, up_weights, gate_weights, output, x.stride(0), x.stride(1), output.stride(0), output.stride(1), L, H, O, \n",
    "                               BLOCK_SIZE_H=BLOCK_SIZE_H, BLOCK_SIZE_O=BLOCK_SIZE_O,num_stages=num_stages)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((16, 64, 256), device='cuda', dtype=torch.float32)\n",
    "out = swiglu(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
