{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def conv2d_kernel(input_ptr: torch.Tensor, kernel_ptr: torch.Tensor, output: torch.Tensor, \n",
    "           height: int, width: int,  stride: int, kH: int, kW: int, max_kH: tl.constexpr, max_kW: tl.constexpr, BLOCK_SIZE: tl.constexpr): \n",
    "    \n",
    "    tile_row = tl.program_id(0)\n",
    "    tile_col = tl.program_id(1)\n",
    "    \n",
    "    out_height = (height - kH) // stride + 1\n",
    "    out_width = (width - kW) // stride + 1\n",
    "    \n",
    "    OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "    OUT_TILE_WIDTH = BLOCK_SIZE - kW + 1\n",
    "    \n",
    "    out_tile_row = tile_row * OUT_TILE_HEIGHT\n",
    "    out_tile_col = tile_col * OUT_TILE_WIDTH\n",
    "    \n",
    "    kernel_row_offset = tl.arange(0, max_kH)\n",
    "    \n",
    "    kernel_col_offset = tl.arange(0, max_kW)\n",
    "\n",
    "    # Create a mask for valid kernel indices.\n",
    "    kernel_mask = (kernel_row_offset[:, None] < kH) & (kernel_col_offset[None, :] < kW)\n",
    "    # Load a full kernel block using masked load.\n",
    "    kernel_block = tl.load(\n",
    "        kernel_ptr + kernel_row_offset[:, None] * kW + kernel_col_offset[None, :],\n",
    "        mask=kernel_mask, other=0.0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for i in tl.range(0, OUT_TILE_HEIGHT):\n",
    "        for j in tl.range(0, OUT_TILE_WIDTH):\n",
    "            # acc = 0.0\n",
    "            out_row = out_tile_row + i\n",
    "            out_col = out_tile_col + j\n",
    "            if out_row < out_height and out_col < out_width:\n",
    "                input_row = out_row * stride\n",
    "                input_col = out_col * stride\n",
    "                \n",
    "                patch_ptr = input_ptr + input_row * width + input_col\n",
    "                input_patch = patch_ptr + kernel_row_offset[:, None] * width + kernel_col_offset[None, :]\n",
    "                mask = (input_row + kernel_row_offset[:, None] < height) & (input_col + kernel_col_offset[None, :] < width)\n",
    "                input_block = tl.load(input_patch, mask = mask, other=0.0)\n",
    "                acc = tl.sum(input_block * kernel_block)\n",
    "                tl.store(output + out_row * out_width + out_col, acc)\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dimensions and hyperparameters.\n",
    "height, width = 34, 34     # Input dimensions.\n",
    "kH, kW = 3, 3                # Kernel dimensions.\n",
    "stride = 1\n",
    "BLOCK_SIZE = 16              # Choose a BLOCK_SIZE such that BLOCK_SIZE > kH and BLOCK_SIZE > kW.\n",
    "\n",
    "# Create dummy tensors.\n",
    "input_tensor = torch.randn(height, width, device='cuda', dtype=torch.float32)\n",
    "kernel_tensor = torch.randn(kH, kW, device='cuda', dtype=torch.float32)\n",
    "out_height = (height - kH) // stride + 1\n",
    "out_width = (width - kW) // stride + 1\n",
    "output_tensor = torch.empty(out_height, out_width, device='cuda', dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute grid dimensions based on tile size.\n",
    "OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "OUT_TILE_WIDTH  = BLOCK_SIZE - kW + 1\n",
    "num_tile_rows = (out_height + OUT_TILE_HEIGHT - 1) // OUT_TILE_HEIGHT\n",
    "num_tile_cols = (out_width + OUT_TILE_WIDTH - 1) // OUT_TILE_WIDTH\n",
    "\n",
    "# The grid is 2D.\n",
    "grid = (num_tile_rows, num_tile_cols)\n",
    "max_kH, max_kW = triton.next_power_of_2(kH), triton.next_power_of_2(kW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_kernel[grid](input_tensor, kernel_tensor, output_tensor,\n",
    "    height, width, stride, kH, kW, max_kH, max_kW, BLOCK_SIZE)\n",
    "\n",
    "torch_output = F.conv2d(input_tensor[None, None, :, :], kernel_tensor[None, None, :, :]).squeeze()\n",
    "torch.allclose(output_tensor, torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
