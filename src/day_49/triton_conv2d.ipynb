{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE': 16}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 32}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 16}, num_stages=4, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 32}, num_stages=4, num_warps=8),                 \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def conv2d_kernel(input_ptr: torch.Tensor, kernel_ptr: torch.Tensor, output: torch.Tensor, \n",
    "           height: int, width: int,  stride: int, kH: int, kW: int, \n",
    "           max_kH: tl.constexpr, max_kW: tl.constexpr, BLOCK_SIZE: tl.constexpr): \n",
    "    \n",
    "    tile_row = tl.program_id(0)\n",
    "    tile_col = tl.program_id(1)\n",
    "    \n",
    "    out_height = (height - kH) // stride + 1\n",
    "    out_width = (width - kW) // stride + 1\n",
    "    \n",
    "    OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "    OUT_TILE_WIDTH = BLOCK_SIZE - kW + 1\n",
    "    \n",
    "    out_tile_row = tile_row * OUT_TILE_HEIGHT\n",
    "    out_tile_col = tile_col * OUT_TILE_WIDTH\n",
    "    \n",
    "    kernel_row_offset = tl.arange(0, max_kH)\n",
    "    \n",
    "    kernel_col_offset = tl.arange(0, max_kW)\n",
    "\n",
    "    # Create a mask for valid kernel indices.\n",
    "    kernel_mask = (kernel_row_offset[:, None] < kH) & (kernel_col_offset[None, :] < kW)\n",
    "    # Load a full kernel block using masked load.\n",
    "    kernel_block = tl.load(\n",
    "        kernel_ptr + kernel_row_offset[:, None] * kW + kernel_col_offset[None, :],\n",
    "        mask=kernel_mask, other=0.0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for i in tl.range(0, OUT_TILE_HEIGHT):\n",
    "        for j in tl.range(0, OUT_TILE_WIDTH):\n",
    "            # acc = 0.0\n",
    "            out_row = out_tile_row + i\n",
    "            out_col = out_tile_col + j\n",
    "            if out_row < out_height and out_col < out_width:\n",
    "                input_row = out_row * stride\n",
    "                input_col = out_col * stride\n",
    "                \n",
    "                patch_ptr = input_ptr + input_row * width + input_col\n",
    "                input_patch = patch_ptr + kernel_row_offset[:, None] * width + kernel_col_offset[None, :]\n",
    "                mask = (input_row + kernel_row_offset[:, None] < height) & (input_col + kernel_col_offset[None, :] < width) & kernel_mask\n",
    "                input_block = tl.load(input_patch, mask = mask, other=0.0)\n",
    "                acc = tl.sum(input_block * kernel_block)\n",
    "                tl.store(output + out_row * out_width + out_col, acc)\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(height: int, width: int, kH: int, kW: int, stride: int, BLOCK_SIZE: int):\n",
    "    # Create dummy tensors.\n",
    "    input_tensor = torch.randn(height, width, device='cuda', dtype=torch.float32)\n",
    "    kernel_tensor = torch.randn(kH, kW, device='cuda', dtype=torch.float32)\n",
    "    out_height = (height - kH) // stride + 1\n",
    "    out_width = (width - kW) // stride + 1\n",
    "    output_tensor = torch.empty(out_height, out_width, device='cuda', dtype=torch.float32)\n",
    "    # Compute grid dimensions based on tile size.\n",
    "    OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "    OUT_TILE_WIDTH  = BLOCK_SIZE - kW + 1\n",
    "    num_tile_rows = (out_height + OUT_TILE_HEIGHT - 1) // OUT_TILE_HEIGHT\n",
    "    num_tile_cols = (out_width + OUT_TILE_WIDTH - 1) // OUT_TILE_WIDTH\n",
    "\n",
    "    # The grid is 2D.\n",
    "    grid = (num_tile_rows, num_tile_cols)\n",
    "    max_kH, max_kW = triton.next_power_of_2(kH), triton.next_power_of_2(kW)\n",
    "    \n",
    "    conv2d_kernel[grid](input_tensor, kernel_tensor, output_tensor,\n",
    "    height, width, stride, kH, kW, max_kH, max_kW, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Triton and Torch conv2d implementations differ\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions and hyperparameters.\n",
    "height, width = 512, 512     # Input dimensions.\n",
    "kH, kW = 3, 3                # Kernel dimensions.\n",
    "stride = 1\n",
    "BLOCK_SIZE = 16              # Choose a BLOCK_SIZE such that BLOCK_SIZE > kH and BLOCK_SIZE > kW.\n",
    "\n",
    "output_tensor = conv2d(height=height, width=width, kH=kH, kW=kW, stride=stride, BLOCK_SIZE=BLOCK_SIZE)\n",
    "torch_output = F.conv2d(input_tensor[None, None, :, :], kernel_tensor[None, None, :, :]).squeeze()\n",
    "if torch.allclose(output_tensor, torch_output, rtol=1e-5, atol=1e-6):\n",
    "    print(\"✅ Triton and Torch conv2d implementations match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch conv2d implementations differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1862, -2.3624,  1.5896,  ..., -3.7066,  1.3915, -0.1006],\n",
       "        [-1.4170, -3.6616, -3.4478,  ...,  2.1552,  0.9247,  0.5250],\n",
       "        [-3.0319,  5.6185, -1.8613,  ...,  1.0411,  0.4177, -2.6132],\n",
       "        ...,\n",
       "        [-3.3992,  6.8019,  3.3175,  ...,  0.6340,  0.2065, -0.1600],\n",
       "        [ 1.6621,  3.1356,  4.1521,  ...,  0.9306, -0.6480,  0.1893],\n",
       "        [ 3.5988,  1.5337,  1.2764,  ..., -0.9792,  1.9511,  1.3613]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4613,  4.0595, -1.2003,  ...,  3.5653,  2.1818, -2.1646],\n",
       "        [ 2.2217, -1.0708, -4.5527,  ...,  0.4244,  0.4346,  3.8623],\n",
       "        [ 0.5882,  1.0266, -2.6549,  ..., -0.3509, -3.8504,  4.2345],\n",
       "        ...,\n",
       "        [-1.3854,  2.0151, -1.3444,  ..., -1.4743, -2.7378,  0.3158],\n",
       "        [-0.3509,  0.1170,  1.7632,  ...,  0.3835, -3.3642,  1.6686],\n",
       "        [-0.1533,  1.1206, -1.5568,  ...,  2.1246, -2.5077, -3.0377]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurate the benchmarks\n",
    "configs = []\n",
    "ref_lib = \"torch\"\n",
    "kernel_sizes = [3, 5, 7, 11]\n",
    "\n",
    "for ksz in seq_lengths:\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"image_size\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[128, 256, 512],  # Different possible values for head_dim\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "            line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"GB/s\",  # Label name for the y-axis\n",
    "            plot_name=f\"conv2d-performance-seq{ksz}-fp32\",  # Name for the plot\n",
    "            args={\"seq_len\": ksz, \"batch_size\": 32, \"num_heads\": 32},\n",
    "        ))\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(head_dim, provider, batch_size, seq_len, num_heads):\n",
    "    # Generate test data\n",
    "    q = torch.randn((batch_size, seq_len, num_heads, head_dim), device=DEVICE, dtype=torch.float32)\n",
    "    k = torch.randn((batch_size, seq_len, num_heads, head_dim), device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    # Position IDs\n",
    "    position_ids = torch.arange(seq_len, device=DEVICE).unsqueeze(0).repeat(batch_size, 1)\n",
    "    \n",
    "    # Precompute cosines and sines\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=DEVICE).float() / head_dim))\n",
    "    \n",
    "    # Initialize cosine and sine tables\n",
    "    cos_table = torch.zeros((seq_len, head_dim // 2), device=DEVICE)\n",
    "    sin_table = torch.zeros((seq_len, head_dim // 2), device=DEVICE)\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        freqs = pos * inv_freq\n",
    "        cos_table[pos] = freqs.cos()\n",
    "        sin_table[pos] = freqs.sin()\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch_apply_rope(q, k, position_ids, head_dim=head_dim, cos=cos_table, sin=sin_table), \n",
    "            quantiles=quantiles\n",
    "        )\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: apply_rope(q, k, position_ids, cos_table, sin_table),\n",
    "            quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate memory bandwidth\n",
    "    # For RoPE:\n",
    "    # - Read Q and K tensors\n",
    "    # - Read cosine and sine tables for each position\n",
    "    # - Write output Q and K tensors\n",
    "    bytes_per_element = 4  # float32\n",
    "    \n",
    "    # Input tensors size: batch_size * seq_len * num_heads * head_dim * 2 (for Q and K)\n",
    "    input_size = batch_size * seq_len * num_heads * head_dim * 2\n",
    "    \n",
    "    # Lookup tables size: seq_len * (head_dim // 2) * 2 (for cos and sin)\n",
    "    lookup_size = seq_len * (head_dim // 2) * 2\n",
    "    \n",
    "    # Position ids size: batch_size * seq_len\n",
    "    pos_ids_size = batch_size * seq_len\n",
    "    \n",
    "    # Output tensors size: same as input\n",
    "    output_size = input_size\n",
    "    \n",
    "    # Total bytes accessed\n",
    "    bytes_accessed = (input_size + lookup_size + pos_ids_size + output_size) * bytes_per_element\n",
    "    \n",
    "    # Convert to GB/s\n",
    "    gb_per_s = lambda ms: bytes_accessed * 1e-9 / (ms * 1e-3)\n",
    "    \n",
    "    return gb_per_s(ms), gb_per_s(max_ms), gb_per_s(min_ms)\n",
    "\n",
    "# Run the benchmark\n",
    "print(benchmark.run(show_plots=True, print_data=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
