{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f\"cuda:{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def transpose_kernel(input_ptr: torch.Tensor, output_ptr: torch.Tensor, \n",
    "              num_rows: int, num_cols: int, \n",
    "              BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    \n",
    "    row_block = tl.program_id(0)\n",
    "    col_block = tl.program_id(1)\n",
    "    # we'll load (BLOCK_SIZE, BLOCK_SIZE) in shared memory and store (BLOCK_SIZE, BLOCK_SIZE) in output.\n",
    "    row_start = row_block * BLOCK_SIZE\n",
    "    col_start = col_block * BLOCK_SIZE\n",
    "    row_offs = tl.arange(0, BLOCK_SIZE)\n",
    "    col_offs = tl.arange(0, BLOCK_SIZE)\n",
    "    rows = row_start + row_offs\n",
    "    cols = col_start + col_offs\n",
    "    input_ptrs = input_ptr + rows[None, :] * num_cols + cols[None, :]\n",
    "    mask = (rows[:, None] < num_rows) & (cols[None, :] < num_cols)\n",
    "    block = tl.load(input_ptrs) # (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    \n",
    "    out_row_start = col_start\n",
    "    out_col_start = row_start\n",
    "    out_rows = out_row_start + col_offs  # Note the swap of row_offs and col_offs\n",
    "    out_cols = out_col_start + row_offs  # Note the swap of row_offs and col_offs\n",
    "    out_ptrs = output_ptr + out_rows[:, None] * num_rows + out_cols[None, :] # Note num_rows here\n",
    "    out_mask = (out_rows[:, None] < num_cols) & (out_cols[None, :] < num_rows) # Note the swap of num_rows and num_cols\n",
    "\n",
    "    tl.store(out_ptrs, block, mask=out_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def transpose_kernel(input_ptr: torch.Tensor, output_ptr: torch.Tensor,\n",
    "              num_rows: int, num_cols: int,\n",
    "              BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, num_stages: tl.constexpr):\n",
    "\n",
    "    row_block = tl.program_id(0)\n",
    "    col_block = tl.program_id(1)\n",
    "\n",
    "    row_offs = tl.arange(0, BLOCK_SIZE_M)\n",
    "    col_offs = tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    # Input indices\n",
    "    rows = row_offs + row_block * BLOCK_SIZE_M\n",
    "    cols = col_offs + col_block * BLOCK_SIZE_N\n",
    "    # we load in transposed manner.\n",
    "    input_ptrs = input_ptr + rows[None, :] * num_cols + cols[:, None]\n",
    "    mask = (rows[None, :] < num_rows) & (cols[:, None] < num_cols)\n",
    "    block = tl.load(input_ptrs, mask=mask)\n",
    "\n",
    "    # Output indices - note the reversal of row and col block and offsets\n",
    "    out_rows = col_block * BLOCK_SIZE_N + col_offs\n",
    "    out_cols = row_block * BLOCK_SIZE_M + row_offs\n",
    "    out_ptrs = output_ptr + out_rows[:, None] * num_cols + out_cols[None, :] # Output has num_cols rows\n",
    "    out_mask = (out_rows[:, None] < num_cols) & (out_cols[None, :] < num_rows) # Output has num_rows cols\n",
    "\n",
    "    tl.store(out_ptrs, block, mask=out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "BLOCK_SIZE_M = 16  # or any appropriate block size\n",
    "BLOCK_SIZE_N = 8\n",
    "num_rows = 64\n",
    "num_cols = 64\n",
    "input_ptr = torch.randn((num_rows, num_cols), dtype=torch.float32, device=DEVICE)\n",
    "output_ptr = torch.zeros((num_cols, num_rows), dtype=torch.float32, device=input_ptr.device) # Output dimensions are swapped\n",
    "num_stages = 1\n",
    "grid = (triton.cdiv(num_rows, BLOCK_SIZE_M), triton.cdiv(num_cols, BLOCK_SIZE_N)) # Grid based on input blocking\n",
    "transpose_kernel[grid](input_ptr, output_ptr, num_rows, num_cols, \n",
    "                       BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N,\n",
    "                       num_stages=num_stages)\n",
    "\n",
    "# Verify the transpose\n",
    "expected_output = input_ptr.T.contiguous()\n",
    "error = torch.mean(torch.abs(expected_output - output_ptr))\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "# You can also visually inspect a small portion\n",
    "if num_rows <= 8 and num_cols <= 8:\n",
    "    print(\"Input:\\n\", input_ptr.cpu().numpy())\n",
    "    print(\"Output:\\n\", output_ptr.cpu().numpy())\n",
    "    print(\"Expected Output:\\n\", expected_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_kernel_np(input: np.ndarray, output: np.ndarray, num_rows: int, num_cols: int, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, row_block: int, col_block: int):\n",
    "    input_row_offsets = np.arange(0, BLOCK_SIZE_M)[:, None]\n",
    "    input_col_offsets = np.arange(0, BLOCK_SIZE_N)[None, :]\n",
    "    \n",
    "    row_ptrs = np.arange(0, BLOCK_SIZE_M)[None, :] + row_block * BLOCK_SIZE_M\n",
    "    col_ptrs = np.arange(0, BLOCK_SIZE_N)[:, None] + col_block * BLOCK_SIZE_N\n",
    "\n",
    "    block = y[row_ptrs, col_ptrs]\n",
    "    out_row_ptrs = np.arange(0, BLOCK_SIZE_N)[:, None] + col_block * BLOCK_SIZE_N\n",
    "    out_col_ptrs = np.arange(0, BLOCK_SIZE_M)[None, :] + row_block * BLOCK_SIZE_M\n",
    "    output[out_row_ptrs, out_col_ptrs] = block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]\n",
      " [16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]\n",
      " [32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47]\n",
      " [48 49 50 51 52 53 54 55]\n",
      " [56 57 58 59 60 61 62 63]]\n",
      "[[ 0  8 16 24 32 40 48 56]\n",
      " [ 1  9 17 25 33 41 49 57]\n",
      " [ 2 10 18 26 34 42 50 58]\n",
      " [ 3 11 19 27 35 43 51 59]\n",
      " [ 4 12 20 28 36 44 52 60]\n",
      " [ 5 13 21 29 37 45 53 61]\n",
      " [ 6 14 22 30 38 46 54 62]\n",
      " [ 7 15 23 31 39 47 55 63]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "BLOCK_SIZE_M, BLOCK_SIZE_N = 4, 2\n",
    "y = np.arange(64).reshape(8, 8)\n",
    "print(y)\n",
    "num_row_blocks = num_rows // BLOCK_SIZE_M\n",
    "num_col_blocks = num_cols // BLOCK_SIZE_N\n",
    "output = np.empty((num_cols, num_rows), dtype=y.dtype)\n",
    "for i in range(0, num_row_blocks):\n",
    "    for j in range(0, num_col_blocks):\n",
    "        transpose_kernel_np(y, output, num_rows, num_cols, BLOCK_SIZE_M, BLOCK_SIZE_N, i, j)\n",
    "\n",
    "print(output)\n",
    "np.alltrue(y.T == output)\n",
    "# row_ptrs = input_row_offsets + row_block * BLOCK_SIZE_M\n",
    "# col_ptrs = input_col_offsets + col_block * BLOCK_SIZE_N\n",
    "# b = block.ravel()\n",
    "# z = out.ravel()\n",
    "# out = np.empty((c, r), dtype=y.dtype)\n",
    "# ri = np.arange(0, BLOCK_SIZE_N)[:, None] * out.shape[1]\n",
    "# ci = np.arange(0, BLOCK_SIZE_M)[None, :]\n",
    "# oi = (ci + ri).ravel()\n",
    "# z = out.ravel()\n",
    "# z[oi] = b[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1017, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "BLOCK_SIZE = 64  # or any appropriate block size\n",
    "input_ptr = torch.randn((BLOCK_SIZE, BLOCK_SIZE), dtype=torch.float32, device=DEVICE)\n",
    "output_ptr = torch.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=torch.float32, device=input_ptr.device)\n",
    "num_rows, num_cols = input_ptr.shape\n",
    "num_stages = 1\n",
    "grid = (triton.cdiv(num_rows, BLOCK_SIZE), triton.cdiv(num_cols, BLOCK_SIZE))\n",
    "transpose_kernel[grid](input_ptr, output_ptr, num_rows, num_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                       num_stages=num_stages)\n",
    "torch.mean(torch.abs(input_ptr.T.contiguous() - output_ptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<triton.compiler.compiler.CompiledKernel at 0x7fe2364c7fa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@triton.jit\n",
    "def reshape_kernel(x_ptr, y_ptr, BLOCK_SIZE: tl.constexpr):\n",
    "    x_offsets = (\n",
    "        tl.arange(0, BLOCK_SIZE)[None, :] * BLOCK_SIZE\n",
    "        + tl.arange(0, BLOCK_SIZE)[:, None]\n",
    "    )\n",
    "    y_offsets = (\n",
    "        tl.arange(0, BLOCK_SIZE)[None, :]\n",
    "        + tl.arange(0, BLOCK_SIZE)[:, None] * BLOCK_SIZE\n",
    "    )\n",
    "    x = tl.load(x_ptr + x_offsets)\n",
    "    tl.store(y_ptr + y_offsets, x)\n",
    "\n",
    "\n",
    "BLOCK_SIZE = 64\n",
    "x = torch.randn((BLOCK_SIZE * BLOCK_SIZE), device=\"cuda\", dtype=torch.float32).view(\n",
    "    (BLOCK_SIZE, BLOCK_SIZE)\n",
    ")\n",
    "y = torch.zeros((BLOCK_SIZE * BLOCK_SIZE), device=\"cuda\", dtype=torch.float32).view(\n",
    "    (BLOCK_SIZE, BLOCK_SIZE)\n",
    ")\n",
    "reshape_kernel[(1,)](x, y, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(x - y.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<triton.compiler.compiler.CompiledKernel at 0x7fa060bbbb80>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1017, device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(input_ptr - output_ptr.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'tensor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_builder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Permutes the dimensions of a tensor.\n",
      "\n",
      "If no permutation is specified, tries to do a (1,0) permutation, i.e. tries\n",
      "to transpose a 2D tensor.\n",
      "\n",
      ":param input: The input tensor.\n",
      ":param dims: The desired ordering of dimensions.  For example,\n",
      "    :code:`(2, 1, 0)` reverses the order dims in a a 3D tensor.\n",
      "\n",
      ":code:`dims` can be passed as a tuple or as individual parameters: ::\n",
      "\n",
      "    # These are equivalent\n",
      "    trans(x, (2, 1, 0))\n",
      "    trans(x, 2, 1, 0)\n",
      "\n",
      ":py:func:`permute` is equivalent to this function, except it doesn't\n",
      "have the special case when no permutation is specified.\n",
      "\n",
      "This function can also be called as a member function on :py:class:`tensor`,\n",
      "as :code:`x.trans(...)` instead of\n",
      ":code:`trans(x, ...)`.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "tl.trans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
