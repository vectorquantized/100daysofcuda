{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "from pprint import pprint, pformat\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"properties={'max_shared_mem': 101376, 'max_num_regs': 65536, \"\n",
      " \"'multiprocessor_count': 64, 'warpSize': 32, 'sm_clock_rate': 1695000, \"\n",
      " \"'mem_clock_rate': 8001000, 'mem_bus_width': 384}\")\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "pprint(f\"{properties=}\", underscore_numbers=True)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def layernorm_kernel(input_ptr: torch.Tensor, output_ptr: torch.Tensor, \n",
    "                     gamma: torch.Tensor, beta: torch.Tensor, \n",
    "                     input_row_stride: int,\n",
    "                     num_rows: int, num_cols: int, \n",
    "                     eps: float,\n",
    "                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    row_idx = tl.program_id(0)\n",
    "    \n",
    "    sum_x = 0.0\n",
    "    sum_squared_x = 0.0\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    output_row_start_ptr = output_ptr + row_idx * input_row_stride\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    for col in tl.range(0, num_cols, BLOCK_SIZE, num_stages=num_stages):\n",
    "        row_ptrs = row_start_ptr + col_offsets + col\n",
    "        mask = (col_offsets + col) < num_cols\n",
    "        x = tl.load(row_ptrs, mask=mask, other=0.0)\n",
    "        sum_x += tl.sum(x, where=mask)\n",
    "        sum_squared_x += t.sum(x * x, where=mask)\n",
    "    \n",
    "    mean = sum_x / num_cols\n",
    "    var = (sum_squared_x / mean) - (mean * mean)\n",
    "    inv_std = tl.rsqrt(var + eps)\n",
    "    \n",
    "    for col in tl.arange(0, num_cols, BLOCK_SIZE, num_stages=num_stages):\n",
    "        row_ptrs = row_start_ptr + col_offsets + col\n",
    "        mask = (col_offsets + col) < num_cols\n",
    "        input_block = tl.load(row_ptrs, mask=mask, other=0.0)\n",
    "        gammas = tl.load(gamma + col, mask=mask, other=0.0)\n",
    "        betas = tl.load(beta + col, mask=mask, other=0.0)\n",
    "        norm = (input_block - mean) * inv_std\n",
    "        output = norm * gammas + betas\n",
    "        tl.store(output_row_start_ptr + col_offsets + col, output, mask=mask)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm(a: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    b = torch.zeros_like(a)\n",
    "    M, N = a.shape\n",
    "    num_stages = 3\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = 128 #triton.next_power_of_2(n_cols)\n",
    "    grid= (M, (N + BLOCK_SIZE - 1) // BLOCK_SIZE)\n",
    "    layernorm_kernel[grid](a, b, gamma, beta,a.stride(0), M, N, eps, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 14:4:\n                     input_row_stride: int,\n                     num_rows: int, num_cols: int, \n                     eps: float,\n                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n    row_idx = tl.program_id(0)\n\n    sum_x = 0.0\n    sum_squared_x = 0.0\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    output_row_start_ptr = output_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    for col in tl.range(0, num_cols, BLOCK_SIZE, num_stages=num_stages):\n    ^\nAssertionError('Loop-carried variable sum_x has initial type fp32 but is re-assigned to <[128], fp32> in loop! Please make sure that the type stays consistent.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m gamma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m512\u001b[39m, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m beta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m512\u001b[39m, device\u001b[38;5;241m=\u001b[39mDEVICE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 5\u001b[0m triton_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m torch_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlayer_norm(A, normalized_shape\u001b[38;5;241m=\u001b[39m(D,), weight\u001b[38;5;241m=\u001b[39mgamma, bias\u001b[38;5;241m=\u001b[39mbeta, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(triton_output, torch_output, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.125\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mlayernorm\u001b[0;34m(a, gamma, beta, eps)\u001b[0m\n\u001b[1;32m      6\u001b[0m BLOCK_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m \u001b[38;5;66;03m#triton.next_power_of_2(n_cols)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m grid\u001b[38;5;241m=\u001b[39m (M, (N \u001b[38;5;241m+\u001b[39m BLOCK_SIZE \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BLOCK_SIZE)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mlayernorm_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m b\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:276\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    278\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 14:4:\n                     input_row_stride: int,\n                     num_rows: int, num_cols: int, \n                     eps: float,\n                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n    row_idx = tl.program_id(0)\n\n    sum_x = 0.0\n    sum_squared_x = 0.0\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    output_row_start_ptr = output_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    for col in tl.range(0, num_cols, BLOCK_SIZE, num_stages=num_stages):\n    ^\nAssertionError('Loop-carried variable sum_x has initial type fp32 but is re-assigned to <[128], fp32> in loop! Please make sure that the type stays consistent.')"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((64, 512), device=DEVICE, dtype=torch.float32)\n",
    "gamma = torch.randn(512, device=DEVICE, dtype=torch.float32)\n",
    "beta = torch.randn(512, device=DEVICE, dtype=torch.float32)\n",
    "triton_output = layernorm(a, gamma, beta, eps=1e-5)\n",
    "torch_output = F.layer_norm(A, normalized_shape=(D,), weight=gamma, bias=beta, eps=1e-5)\n",
    "if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
