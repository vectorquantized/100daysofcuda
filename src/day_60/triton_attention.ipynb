{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f\"cuda:{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def attention_kernel(Q_ptr: torch.Tensor, \n",
    "                     K_ptr: torch.Tensor, \n",
    "                     V_ptr: torch.Tensor, \n",
    "                     output_ptr: torch.Tensor,\n",
    "                     batch_stride: int, seq_stride: int,\n",
    "                     head_stride: int,\n",
    "                     B: int, L: int, heads: int, d_k: int, \n",
    "                     BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
    "    \n",
    "    pid_BH = tl.program_id(0)\n",
    "    batch_idx = pid_BH // heads\n",
    "    head_idx = pid_BH % heads\n",
    "    \n",
    "    seq_idx = tl.program_id(1)\n",
    "    pid_block = tl.program_id(2)\n",
    "    \n",
    "    num_blocks = tl.cdiv(d_k, BLOCK_SIZE)\n",
    "    offs_row = pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    offs_col = tl.arange(0, BLOCK_SIZE)\n",
    "    q_base_idx = batch_idx * batch_stride + head_idx * head_stride + seq_idx * seq_stride\n",
    "    k_base_idx = batch_idx * batch_stride + head_idx * head_stride + seq_idx * seq_stride\n",
    "    v_base_idx = batch_idx * batch_stride + head_idx * head_stride + seq_idx * seq_stride\n",
    "    Q_start = Q_ptr + q_base_idx + offs_row[:, None] * d_k + offs_col[None, :]\n",
    "    K_start = K_ptr + k_base_idx + offs_row[:, None] + offs_col[None, :] * L\n",
    "    V_start = V_ptr + v_base_idx + offs_row[:, None] * d_k + offs_col[None, :]\n",
    "    out = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "    for k in tl.range(0, num_blocks, num_stages=num_stages):\n",
    "        mask = (offs_row  + k * BLOCK_SIZE < d_k)[None, :]\n",
    "        Q_tile_ptrs = Q_start + k * BLOCK_SIZE\n",
    "        K_tile_ptrs = K_start + k * BLOCK_SIZE\n",
    "        q = tl.load(Q_tile_ptrs, mask=mask, other=0.0)\n",
    "        k_t = tl.load(K_tile_ptrs, mask=mask, other=0.0)\n",
    "        out += tl.dot(q, k_t)\n",
    "    \n",
    "    out *= tl.rsqrt(d_k)\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
