{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "from pprint import pprint\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"properties={'max_shared_mem': 101376, 'max_num_regs': 65536, \"\n",
      " \"'multiprocessor_count': 64, 'warpSize': 32, 'sm_clock_rate': 1695000, \"\n",
      " \"'mem_clock_rate': 8001000, 'mem_bus_width': 384}\")\n"
     ]
    }
   ],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "pprint(f\"{properties=}\", underscore_numbers=True)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _swiglu_forward_kernel(input_ptr: torch.Tensor, up_ptr: torch.Tensor, gate_ptr:torch.Tensor,\n",
    "                         output_ptr: torch.Tensor, input_batch_stride: int, input_seq_stride: int, \n",
    "                         output_batch_stride: int, output_seq_stride: int,\n",
    "                         L: int, H: int, O:int, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_O: tl.constexpr):\n",
    "    '''\n",
    "    Triton kernel for gelu.\n",
    "    define silu(x) = x * sigmoid(x)\n",
    "    gelu(x, up, gate) = up(x) * silu(gate(x))\n",
    "    x \\in R^{B x L x H}\n",
    "    up \\in R^{O x H} \\implies up(x) \\in R^{B x L x O}\n",
    "    gate \\in R^{O x H} \\implies gate(x) \\in R^{B x L x O}\n",
    "    gelu \\in R^{B x L x O}\n",
    "    Args:\n",
    "        input_ptr: Pointer to the input, shape: (B, L, H)\n",
    "        up_ptr: Pointer to the weights of Linear layer, shape: (O, H)\n",
    "        gate_ptr: Pointer to the weights of the Linear layer, shaoe: (O, H)\n",
    "        output_ptr: Pointer to output, shape: (B, L, O)\n",
    "        input_batch_stride: number of elements we move to reach next batch in the input\n",
    "        input_seq_stride: number of elements we move to reach next sequence in the input\n",
    "        output_batch_stride: number of elements we move to reach next batch in the output\n",
    "        output_seq_stride: number of elements we move to reach next sequence in the output\n",
    "        L: Sequence Length\n",
    "        H: Embedding Dimension\n",
    "        O: Output Dimension\n",
    "    '''\n",
    "    pid_BL = tl.program_id(0)\n",
    "    batch_idx = pid_BL // L\n",
    "    seq_idx = pid_BL % L\n",
    "    input_base_idx = batch_idx * input_batch_stride + seq_idx * input_seq_stride \n",
    "    output_base_idx = batch_idx * output_batch_stride + seq_idx * output_seq_stride\n",
    "    input_start_ptr = input_ptr + input_base_idx\n",
    "    output_start_ptr = output_ptr + output_base_idx\n",
    "    \n",
    "    pid = tl.program_id(1)\n",
    "    num_pid_h = tl.cdiv(H, BLOCK_SIZE_H)\n",
    "    num_pid_o = tl.cdiv(O, BLOCK_SIZE_O)\n",
    "    pid_h = pid // num_pid_o\n",
    "    pid_o = pid % num_pid_o\n",
    "    offs_ah = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n",
    "    offs_bo = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n",
    "    offs_h = tl.arange(0, BLOCK_SIZE_H)\n",
    "    a_ptrs = input_start_ptr + offs_ah # shape (BLOCK_SIZE_H)\n",
    "    b_ptrs = gate_ptr + offs_bo[:, None] * H  + offs_h[None, :] # shape: (BLOCK_SIZE_O, BLOCK_SIZE_H)\n",
    "    c_ptrs = up_ptr + offs_bo[:, None] * H + offs_h[None, :]\n",
    "    gated = tl.zeros((BLOCK_SIZE_O,), dtype=tl.float32)\n",
    "    up = tl.zeros((BLOCK_SIZE_O, ), dtype=tl.float32)\n",
    "    \n",
    "    for k in tl.range(0, tl.cdiv(H, BLOCK_SIZE_H)):\n",
    "        offs_h = tl.arange(0, BLOCK_SIZE_H)\n",
    "        a_tile_ptrs = a_ptrs + k * BLOCK_SIZE_H\n",
    "        mask_a = (offs_ah + k * BLOCK_SIZE_H < H)\n",
    "        b_tile_ptrs = b_ptrs + k * BLOCK_SIZE_H\n",
    "        mask_b = (offs_h + k * BLOCK_SIZE_H < H)[None, :]\n",
    "        c_tile_ptrs = c_ptrs + k * BLOCK_SIZE_H\n",
    "        a = tl.load(a_tile_ptrs, mask=mask_a, other=0.0)\n",
    "        b = tl.load(b_tile_ptrs, mask=mask_b, other=0.0)\n",
    "        c = tl.load(c_tile_ptrs, mask=mask_b, other=0.0)\n",
    "        gated += tl.sum(b * a, axis=1)\n",
    "        up += tl.sum(c * a, axis=1)\n",
    "    \n",
    "    \n",
    "    silu = gated * tl.sigmoid(gated)\n",
    "    output = up * silu\n",
    "    output_ptrs = output_start_ptr + offs_bo\n",
    "    mask_output = offs_bo < O\n",
    "    tl.store(output_ptrs, output, mask=mask_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swiglu(x: torch.Tensor, up_weights: torch.Tensor, gate_weights: torch.Tensor, O: int) -> torch.Tensor:\n",
    "    B, L, H = x.shape\n",
    "    \n",
    "    # # Create weight matrices (these would normally be module parameters)\n",
    "    # up_weights = torch.empty((O, H), dtype=torch.float32, device=x.device)\n",
    "    # gate_weights = torch.empty((O, H), dtype=torch.float32, device=x.device)\n",
    "    \n",
    "    # # Initialize weights (in a real implementation, these would be trained parameters)\n",
    "    # # This is just placeholder initialization\n",
    "    # torch.nn.init.kaiming_uniform_(up_weights)\n",
    "    # torch.nn.init.kaiming_uniform_(gate_weights)\n",
    "    \n",
    "    num_stages = 8\n",
    "    BLOCK_SIZE_H = 256\n",
    "    BLOCK_SIZE_O = 512\n",
    "    grid = lambda META: (B * L, triton.cdiv(H, META['BLOCK_SIZE_H']) * triton.cdiv(O, META['BLOCK_SIZE_O']), )\n",
    "    output = torch.empty((B, L, O), dtype=torch.float32, device=x.device)\n",
    "    _swiglu_forward_kernel[grid](x, up_weights, gate_weights, output, x.stride(0), x.stride(1), output.stride(0), output.stride(1), L, H, O, \n",
    "                               BLOCK_SIZE_H=BLOCK_SIZE_H, BLOCK_SIZE_O=BLOCK_SIZE_O,num_stages=num_stages)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swiglu(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.gate = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    \n",
    "    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run forward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            input_batch: a torch.Tensor of shape: (B, L, D)\n",
    "        \n",
    "        Returns:\n",
    "            a torch.Tensor of shape: (B, L, O)\n",
    "        \n",
    "        \"\"\"\n",
    "        gated = self.gate(input_batch)\n",
    "        silu = gated * F.sigmoid(gated)\n",
    "        return self.up(input_batch) * silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, D, O = 16, 64, 256, 1024\n",
    "x = torch.randn((B, L, D), device='cuda', dtype=torch.float32)\n",
    "torch_swiglu = Swiglu(D, O).to(x.device)\n",
    "up_weights = torch_swiglu.up.weight\n",
    "gate_weights = torch_swiglu.gate.weight\n",
    "torch_output = torch_swiglu(x)\n",
    "triton_output = swiglu(x, up_weights, gate_weights, O)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3280e-08, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(torch_output - triton_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "ref_lib = \"torch\"\n",
    "output_sizes = [2048]\n",
    "L = 64\n",
    "for o in output_sizes:\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"H\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[2**i for i in range(8, 20)],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[ref_lib.lower(), \"triton\"] , # Label name for the lines\n",
    "            line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"swiglu-performance-\" +\n",
    "            (\"fp32\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={\"L\": L, \"O\": o},\n",
    "        ))\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(H, provider, L, O):\n",
    "    B = 16\n",
    "    a = torch.randn((B, L, H), device=DEVICE, dtype=torch.float32)\n",
    "    torch_swiglu = Swiglu(H, O).to(a.device)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch_swiglu(a), quantiles=quantiles)\n",
    "    up_weights, gate_weights = torch_swiglu.up.weight, torch_swiglu.gate.weight\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: swiglu(a, up_weights, gate_weights, O), quantiles=quantiles)\n",
    "    # Calculate memory bandwidth: bytes_accessed / runtime_in_seconds\n",
    "    # Each float32 element is 4 bytes\n",
    "    bytes_accessed = B * L * H * 4 # Input + Output + gamma\n",
    "    gb_per_s = lambda ms: bytes_accessed * 1e-9 / (ms * 1e-3)\n",
    "    \n",
    "    return gb_per_s(ms), gb_per_s(max_ms), gb_per_s(min_ms)\n",
    "\n",
    "\n",
    "print(benchmark.run(show_plots=True, print_data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
