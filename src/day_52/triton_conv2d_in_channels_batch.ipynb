{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE': 8}, num_stages=3, num_warps=1),\n",
    "        triton.Config({'BLOCK_SIZE': 4}, num_stages=3, num_warps=1),\n",
    "        triton.Config({'BLOCK_SIZE': 16}, num_stages=3, num_warps=1),\n",
    "        triton.Config({'BLOCK_SIZE': 8}, num_stages=8, num_warps=1),\n",
    "        triton.Config({'BLOCK_SIZE': 4}, num_stages=8, num_warps=1),\n",
    "        triton.Config({'BLOCK_SIZE': 32}, num_stages=8, num_warps=1),\n",
    "    ],\n",
    "    key=['height', 'width', 'stride', 'kH', 'kW'],\n",
    ")\n",
    "@triton.jit\n",
    "def conv2d_kernel(input_ptr: torch.Tensor, kernel_ptr: torch.Tensor, output: torch.Tensor, \n",
    "                  batch_stride: int, in_channel_stride: int, channels: int, height: int, width: int, \n",
    "                  k_in_channel_stride: int, stride: int, kH: int, kW: int, \n",
    "                  max_kH: tl.constexpr, max_kW: tl.constexpr, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr): \n",
    "    \n",
    "    batch_idx = tl.program_id(0)\n",
    "    tile_row = tl.program_id(1)\n",
    "    tile_col = tl.program_id(2)\n",
    "    \n",
    "    out_height = (height - kH) // stride + 1\n",
    "    out_width = (width - kW) // stride + 1\n",
    "    \n",
    "    OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "    OUT_TILE_WIDTH = BLOCK_SIZE - kW + 1\n",
    "    \n",
    "    out_tile_row = tile_row * OUT_TILE_HEIGHT\n",
    "    out_tile_col = tile_col * OUT_TILE_WIDTH\n",
    "    \n",
    "    kernel_row_offset = tl.arange(0, max_kH)\n",
    "    \n",
    "    kernel_col_offset = tl.arange(0, max_kW)\n",
    "\n",
    "    # Create a mask for valid kernel indices.\n",
    "    kernel_mask = (kernel_row_offset[:, None] < kH) & (kernel_col_offset[None, :] < kW)\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in tl.range(0, OUT_TILE_HEIGHT, num_stages=num_stages):\n",
    "        for j in tl.range(0, OUT_TILE_WIDTH, num_stages=num_stages):\n",
    "            \n",
    "            out_row = out_tile_row + i\n",
    "            out_col = out_tile_col + j\n",
    "            if out_row < out_height and out_col < out_width:\n",
    "                acc = 0.0\n",
    "                input_row = out_row * stride\n",
    "                input_col = out_col * stride\n",
    "                for c in tl.range(0, channels):\n",
    "                    # Load a full kernel block using masked load.\n",
    "                    kernel_block = tl.load(\n",
    "                        kernel_ptr + c * k_in_channel_stride + kernel_row_offset[:, None] * kW + kernel_col_offset[None, :],\n",
    "                        mask=kernel_mask, other=0.0\n",
    "                    )\n",
    "                    patch_ptr = input_ptr + batch_idx * batch_stride + c * in_channel_stride + input_row * width + input_col\n",
    "                    input_patch = patch_ptr + kernel_row_offset[:, None] * width + kernel_col_offset[None, :]\n",
    "                    mask = (input_row + kernel_row_offset[:, None] < height) & (input_col + kernel_col_offset[None, :] < width) & kernel_mask\n",
    "                    input_block = tl.load(input_patch, mask = mask, other=0.0)\n",
    "                    acc += tl.sum(input_block * kernel_block)\n",
    "                tl.store(output + batch_idx * batch_stride + out_row * out_width + out_col, acc)\n",
    "                    \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_tensor: torch.Tensor, kernel_tensor: torch.Tensor,\n",
    "           num_batch: int, channels: int, height: int, width: int, kH: int, kW: int, stride: int):\n",
    "    \n",
    "    out_height = (height - kH) // stride + 1\n",
    "    out_width = (width - kW) // stride + 1\n",
    "    output_tensor = torch.empty(num_batch, out_height, out_width, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    def grid_fn(meta):\n",
    "        # Extract the BLOCK_SIZE from the current configuration\n",
    "        block_size = meta['BLOCK_SIZE']\n",
    "        out_tile_height = block_size - kH + 1\n",
    "        out_tile_width = block_size - kW + 1\n",
    "        \n",
    "        # Ensure tiles are at least 1x1\n",
    "        out_tile_height = max(1, out_tile_height)\n",
    "        out_tile_width = max(1, out_tile_width)\n",
    "        \n",
    "        num_tile_rows = (out_height + out_tile_height - 1) // out_tile_height\n",
    "        num_tile_cols = (out_width + out_tile_width - 1) // out_tile_width\n",
    "        \n",
    "        return (num_batch, num_tile_rows, num_tile_cols)\n",
    "#     # Compute grid dimensions based on tile size.\n",
    "#     OUT_TILE_HEIGHT = BLOCK_SIZE - kH + 1\n",
    "#     OUT_TILE_WIDTH  = BLOCK_SIZE - kW + 1\n",
    "#     num_tile_rows = (out_height + OUT_TILE_HEIGHT - 1) // OUT_TILE_HEIGHT\n",
    "#     num_tile_cols = (out_width + OUT_TILE_WIDTH - 1) // OUT_TILE_WIDTH\n",
    "\n",
    "    # The grid is 2D.\n",
    "    grid = grid_fn\n",
    "    max_kH, max_kW = triton.next_power_of_2(kH), triton.next_power_of_2(kW)\n",
    "    \n",
    "    conv2d_kernel[grid](input_tensor, kernel_tensor, output_tensor,\n",
    "                        input_tensor.stride(0), input_tensor.stride(1), channels, height, width,\n",
    "                        kernel_tensor.stride(0), stride, kH, kW, max_kH, max_kW)\n",
    "    \n",
    "    return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Triton and Torch conv2d implementations differ\n"
     ]
    }
   ],
   "source": [
    "# Example dimensions and hyperparameters.\n",
    "num_batch, channels, height, width = 16, 2, 1024, 1024     # Input dimensions.\n",
    "kH, kW = 3, 3                # Kernel dimensions.\n",
    "stride = 1\n",
    "input_tensor = torch.randn(num_batch, channels, height, width, device='cuda', dtype=torch.float32)\n",
    "kernel_tensor = torch.randn(channels, kH, kW, device='cuda', dtype=torch.float32)\n",
    "output_tensor = conv2d(input_tensor, kernel_tensor, num_batch=num_batch, channels=channels, height=height, width=width, kH=kH, kW=kW, stride=stride)\n",
    "torch_output = F.conv2d(input_tensor, kernel_tensor[None, :, :, :]).squeeze()\n",
    "if torch.allclose(output_tensor, torch_output, rtol=1e-5, atol=1e-5):\n",
    "    print(\"✅ Triton and Torch conv2d implementations match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch conv2d implementations differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1022, 1022])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.7225,   1.4697,   3.0545,  ...,  -3.7509,   6.4817,  -1.3554],\n",
       "        [ -0.4880,  -7.4489,   7.6456,  ...,  -8.1337,   1.3480,   6.2260],\n",
       "        [  5.7238,   0.9814,  -1.0525,  ...,   7.4619,  -2.2007,   1.6040],\n",
       "        ...,\n",
       "        [ -3.8167,  -8.5957,   3.1965,  ...,  10.9977,  -3.0231, -10.4279],\n",
       "        [ -4.3272,   1.7448,  -0.3093,  ..., -10.2087,   5.7788,  -2.2536],\n",
       "        [ -0.7535,   4.3858,   5.8589,  ...,  -0.9185,   7.4303,  -6.3129]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gb_per_s(ms), gb_per_s(max_ms), gb_per_s(min_ms)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Run the benchmark\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/testing.py:349\u001b[0m, in \u001b[0;36mMark.run\u001b[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<html><body>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bench \u001b[38;5;129;01min\u001b[39;00m benchmarks:\n\u001b[0;32m--> 349\u001b[0m     result_dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[1;32m    351\u001b[0m         html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image src=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbench\u001b[38;5;241m.\u001b[39mplot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/testing.py:292\u001b[0m, in \u001b[0;36mMark._run\u001b[0;34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[0m\n\u001b[1;32m    290\u001b[0m row_mean, row_min, row_max \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m bench\u001b[38;5;241m.\u001b[39mline_vals:\n\u001b[0;32m--> 292\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_arg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwrags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         y_mean, y_min, y_max \u001b[38;5;241m=\u001b[39m ret\n",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(size, provider, kH, kW, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     26\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 27\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m kernel_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(channels, kH, kW, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     29\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Configurate the benchmarks\n",
    "configs = []\n",
    "ref_lib = \"torch\"\n",
    "kernel_sizes = [3]\n",
    "sizes = [128, 256, 512, 1024, 2048]  # Use same values for both height and width\n",
    "batch_size = 32\n",
    "for ksz in kernel_sizes:\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"size\"],  # Single x-axis parameter that will be used for both height and width\n",
    "            x_vals=sizes,     # Different possible values for the size\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "            line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"GB/s\",  # Label name for the y-axis\n",
    "            plot_name=f\"conv2d-performance-k{ksz}-fp32\",  # Name for the plot\n",
    "            args={\"kH\": ksz, \"kW\": ksz, \"batch_size\": batch_size},\n",
    "        ))\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(size, provider, kH, kW, batch_size):\n",
    "    # Use size for both height and width\n",
    "    height = size\n",
    "    width = size\n",
    "    channels = 3\n",
    "    stride = 1\n",
    "    input_tensor = torch.randn(batch_size, channels, height, width, device='cuda', dtype=torch.float32)\n",
    "    kernel_tensor = torch.randn(channels, kH, kW, device='cuda', dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: F.conv2d(input_tensor, kernel_tensor[None, :, :, :]), \n",
    "            quantiles=quantiles\n",
    "        )\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: conv2d(input_tensor, kernel_tensor, num_batch=batch_size, channels=channels, height=height, width=width, kH=kH, kW=kW, stride=stride),\n",
    "            quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate FLOPS\n",
    "    # For each output element, we do:\n",
    "    # - channels * kH * kW multiplications\n",
    "    # - (channels * kH * kW - 1) additions\n",
    "    # Total ops per output element: ~2 * channels * kH * kW (multiply-add)\n",
    "    ops_per_output_element = 2 * channels * kH * kW\n",
    "    total_output_elements = batch_size * out_height * out_width\n",
    "    total_flops = ops_per_output_element * total_output_elements\n",
    "    \n",
    "    # Convert to TFLOPS (tera FLOPS)\n",
    "    tflops = lambda ms: total_flops * 1e-12 / (ms * 1e-3)\n",
    "    \n",
    "    # Calculate memory bandwidth (for reference)\n",
    "    bytes_per_element = 4  # float32\n",
    "    input_size = batch_size * channels * height * width\n",
    "    kernel_size = channels * kH * kW\n",
    "    output_size = batch_size * out_height * out_width\n",
    "    bytes_accessed = (input_size + kernel_size + output_size) * bytes_per_element\n",
    "    gb_per_s = lambda ms: bytes_accessed * 1e-9 / (ms * 1e-3)\n",
    "    \n",
    "    # Create a dual-metric benchmark result\n",
    "    # Primary metric: TFLOPS\n",
    "    result = tflops(ms), tflops(max_ms), tflops(min_ms)\n",
    "    \n",
    "    # Print memory bandwidth for reference\n",
    "    bandwidth = gb_per_s(ms)\n",
    "    print(f\"Size: {size}, Provider: {provider}, TFLOPS: {result[0]:.2f}, Memory BW: {bandwidth:.2f} GB/s\")\n",
    "    \n",
    "    # Compute arithmetic intensity (FLOPS/byte)\n",
    "    arithmetic_intensity = total_flops / bytes_accessed\n",
    "    print(f\"  Arithmetic Intensity: {arithmetic_intensity:.2f} FLOPS/byte\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the benchmark\n",
    "print(benchmark.run(show_plots=True, print_data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_kernel.best_config.num_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
