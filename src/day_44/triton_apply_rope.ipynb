{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def pack_even_odd(q_even: tl.tensor, q_odd: tl.tensor, BLOCK_SIZE: tl.constexpr, DTYPE: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Packs even and odd parts into a flattened 1D tensor.\n",
    "    Assumes q_even and q_odd have shape (BLOCK_SIZE,).\n",
    "    \"\"\"\n",
    "    # Create a temporary tensor with shape (BLOCK_SIZE, 2)\n",
    "    q_temp = tl.zeros((BLOCK_SIZE, 2), dtype==DTYPE)\n",
    "    # Loop over the BLOCK_SIZE dimension to assign even and odd values\n",
    "    for i in range(BLOCK_SIZE):\n",
    "        q_temp[i, 0] = q_even[i]\n",
    "        q_temp[i, 1] = q_odd[i]\n",
    "    # Flatten the tensor to shape (BLOCK_SIZE * 2,)\n",
    "    return tl.reshape(q_temp, (-1,))\n",
    "\n",
    "@triton.jit\n",
    "def apply_rope_kernel(Q: tl.tensor, K: tl.tensor, \n",
    "                      Q_out: tl.tensor, K_out: tl.tensor,\n",
    "                      pos_id: tl.tensor, cosines: tl.tensor, sines: tl.tensor,\n",
    "                      seq_length: int, num_heads: int, head_dim: int,\n",
    "                      batch_stride: int, seq_stride: int, head_stride: int, \n",
    "                      BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr, DTYPE: tl.constexpr):\n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_seq   = tl.program_id(1)\n",
    "    pid_head  = tl.program_id(2)\n",
    "    \n",
    "    pos_ids = tl.load(pos_id + pid_batch * seq_length + pid_seq)\n",
    "    base_offset = pid_batch * batch_stride + pid_seq * seq_stride + pid_head * head_stride\n",
    "    Q_start_ptr = Q + base_offset\n",
    "    K_start_ptr = K + base_offset\n",
    "    Q_out_ptr   = Q_out + base_offset\n",
    "    K_out_ptr   = K_out + base_offset\n",
    "    \n",
    "    for k in range(0, tl.cdiv(num_heads, BLOCK_SIZE * 2)):\n",
    "        dim_offsets = k * (BLOCK_SIZE * 2) + tl.arange(0, BLOCK_SIZE * 2)\n",
    "        mask = dim_offsets < head_dim\n",
    "        \n",
    "        Q_loaded = tl.load(Q_start_ptr + dim_offsets, mask=mask)\n",
    "        K_loaded = tl.load(K_start_ptr + dim_offsets, mask=mask)\n",
    "        \n",
    "        # Split into even and odd parts using our computed indices manually\n",
    "        q_even = tl.zeros((BLOCK_SIZE,), dtype=DTYPE)\n",
    "        q_odd  = tl.zeros((BLOCK_SIZE,), dtype=DTYPE)\n",
    "        k_even = tl.zeros((BLOCK_SIZE,), dtype=DTYPE)\n",
    "        k_odd  = tl.zeros((BLOCK_SIZE,), dtype=DTYPE)\n",
    "        \n",
    "        # Load even and odd components manually\n",
    "        for i in range(BLOCK_SIZE):\n",
    "            even_idx = i * 2\n",
    "            odd_idx = even_idx + 1\n",
    "            q_even[i] = Q_loaded[even_idx]\n",
    "            q_odd[i]  = Q_loaded[odd_idx]\n",
    "            k_even[i] = K_loaded[even_idx]\n",
    "            k_odd[i]  = K_loaded[odd_idx]\n",
    "        \n",
    "        # Compute cosine and sine offsets and load corresponding values\n",
    "        cos_offset = cosines + pos_ids * (head_dim // 2)\n",
    "        sin_offset = sines + pos_ids * (head_dim // 2)\n",
    "        offsets = k * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "        cos_mask = offsets < (head_dim // 2)\n",
    "        cos_values = tl.load(cos_offset + offsets, mask=cos_mask)\n",
    "        sin_values = tl.load(sin_offset + offsets, mask=cos_mask)\n",
    "        \n",
    "        # Compute new values using the rotary transformation\n",
    "        q_even_out = q_even * cos_values - q_odd * sin_values\n",
    "        q_odd_out  = q_even * sin_values + q_odd * cos_values\n",
    "        k_even_out = k_even * cos_values - k_odd * sin_values\n",
    "        k_odd_out  = k_even * sin_values + k_odd * cos_values\n",
    "        \n",
    "        # Use the helper function to pack even and odd outputs into flat arrays.\n",
    "        q_flat = pack_even_odd(q_even_out, q_odd_out, BLOCK_SIZE)\n",
    "        k_flat = pack_even_odd(k_even_out, k_odd_out, BLOCK_SIZE)\n",
    "        \n",
    "        # Store the results\n",
    "        tl.store(Q_out_ptr + dim_offsets, q_flat, mask=mask)\n",
    "        tl.store(K_out_ptr + dim_offsets, k_flat, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(q: torch.Tensor, k: torch.Tensor, pos_ids: torch.Tensor, cosines: torch.Tensor, sines: torch.Tensor):\n",
    "    q_out = torch.zeros_like(q)\n",
    "    k_out = torch.zeros_like(k)\n",
    "    B, L, H, D = q.shape\n",
    "    num_stages = 8\n",
    "    BLOCK_SIZE = 32\n",
    "    \n",
    "    # Grid dimensions should be (batch, sequence_length, num_heads)\n",
    "    grid = (B, L, H)\n",
    "    \n",
    "    apply_rope_kernel[grid](\n",
    "        q, k, cosines, sines, pos_ids,  # Input tensors\n",
    "        q_out, k_out,                   # Output tensors\n",
    "        L, H, D,                        # Dimensions\n",
    "        q.stride(0), q.stride(1), q.stride(2),  # Strides\n",
    "        BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages,\n",
    "        DTYPE=tl.float32\n",
    "    )\n",
    "    \n",
    "    return q_out, k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 26:17:\n    Q_out_ptr   = Q_out + base_offset\n    K_out_ptr   = K_out + base_offset\n\n    for k in range(0, tl.cdiv(num_heads, BLOCK_SIZE * 2)):\n        dim_offsets = k * (BLOCK_SIZE * 2) + tl.arange(0, BLOCK_SIZE * 2)\n        mask = dim_offsets < head_dim\n\n        Q_loaded = tl.load(Q_start_ptr + dim_offsets, mask=mask)\n        K_loaded = tl.load(K_start_ptr + dim_offsets, mask=mask)\n\n        # Split into even and odd parts using our computed indices manually\n        q_even = tl.zeros((BLOCK_SIZE,), dtype=Q.dtype)\n                 ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/core.py:35\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_builder` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/core.py:1223\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, value, dtype, _builder)\u001b[0m\n\u001b[1;32m   1222\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _constexpr_to_value(dtype)\n\u001b[0;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msemantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/language/semantic.py:527\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, value, dtype, builder)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 527\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_null_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Not implemented",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31mCompilationError\u001b[0m: at 10:11:\ndef zeros(shape, dtype):\n    \"\"\"\n    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n\n    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n    :type shape: tuple of ints\n    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n    :type dtype: DType\n    \"\"\"\n    return core.full(shape, 0, dtype)\n           ^",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK max difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(triton_k_out\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtorch_k_out)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtest_apply_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 54\u001b[0m, in \u001b[0;36mtest_apply_rope\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     sin_table[pos] \u001b[38;5;241m=\u001b[39m freqs\u001b[38;5;241m.\u001b[39msin()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Run your Triton version\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m triton_q_out, triton_k_out \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Run PyTorch reference\u001b[39;00m\n\u001b[1;32m     57\u001b[0m torch_q_out, torch_k_out \u001b[38;5;241m=\u001b[39m torch_apply_rope(q, k, position_ids, head_dim)\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mapply_rope\u001b[0;34m(q, k, pos_ids, cosines, sines)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Grid dimensions should be (batch, sequence_length, num_heads)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m grid \u001b[38;5;241m=\u001b[39m (B, L, H)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mapply_rope_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Input tensors\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Output tensors\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Dimensions\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Strides\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stages\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_out, k_out\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:276\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    278\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda/lib/python3.10/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 26:17:\n    Q_out_ptr   = Q_out + base_offset\n    K_out_ptr   = K_out + base_offset\n\n    for k in range(0, tl.cdiv(num_heads, BLOCK_SIZE * 2)):\n        dim_offsets = k * (BLOCK_SIZE * 2) + tl.arange(0, BLOCK_SIZE * 2)\n        mask = dim_offsets < head_dim\n\n        Q_loaded = tl.load(Q_start_ptr + dim_offsets, mask=mask)\n        K_loaded = tl.load(K_start_ptr + dim_offsets, mask=mask)\n\n        # Split into even and odd parts using our computed indices manually\n        q_even = tl.zeros((BLOCK_SIZE,), dtype=Q.dtype)\n                 ^"
     ]
    }
   ],
   "source": [
    "# For comparing with native PyTorch implementation\n",
    "def torch_apply_rope(q, k, position_ids, head_dim):\n",
    "    # Create position embeddings\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=q.device).float() / head_dim))\n",
    "    \n",
    "    # Get sin and cos values\n",
    "    batch_size, seq_len = position_ids.shape\n",
    "    freqs = torch.einsum(\"b l, d -> b l d\", position_ids.float(), inv_freq)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = emb.cos()[:, :, None, :].repeat(1, 1, q.shape[2], 1)\n",
    "    sin = emb.sin()[:, :, None, :].repeat(1, 1, q.shape[2], 1)\n",
    "    \n",
    "    # Apply rotation\n",
    "    q_out = torch.cat([-q[..., 1::2], q[..., ::2]], dim=-1).reshape(q.shape)\n",
    "    k_out = torch.cat([-k[..., 1::2], k[..., ::2]], dim=-1).reshape(k.shape)\n",
    "    \n",
    "    q_out = q * cos + q_out * sin\n",
    "    k_out = k * cos + k_out * sin\n",
    "    \n",
    "    return q_out, k_out\n",
    "\n",
    "# Test function\n",
    "def test_apply_rope():\n",
    "    # Set up test data\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    num_heads = 4\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Generate test data\n",
    "    q = torch.randn((batch_size, seq_len, num_heads, head_dim), device=device, dtype=torch.float32)\n",
    "    k = torch.randn((batch_size, seq_len, num_heads, head_dim), device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Position IDs\n",
    "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "    \n",
    "    # Precompute cosines and sines\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))\n",
    "    \n",
    "    # Initialize cosine and sine tables\n",
    "    cos_table = torch.zeros((seq_len, head_dim // 2), device=device)\n",
    "    sin_table = torch.zeros((seq_len, head_dim // 2), device=device)\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        freqs = pos * inv_freq\n",
    "        cos_table[pos] = freqs.cos()\n",
    "        sin_table[pos] = freqs.sin()\n",
    "    \n",
    "    # Run your Triton version\n",
    "    triton_q_out, triton_k_out = apply_rope(q, k, position_ids, cos_table, sin_table)\n",
    "    \n",
    "    # Run PyTorch reference\n",
    "    torch_q_out, torch_k_out = torch_apply_rope(q, k, position_ids, head_dim)\n",
    "    \n",
    "    # Compare results\n",
    "    q_match = torch.allclose(triton_q_out, torch_q_out, atol=1e-5, rtol=1e-5)\n",
    "    k_match = torch.allclose(triton_k_out, torch_k_out, atol=1e-5, rtol=1e-5)\n",
    "    \n",
    "    if q_match and k_match:\n",
    "        print(\"✅ Triton and Torch RoPE implementations match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch RoPE implementations differ\")\n",
    "        if not q_match:\n",
    "            print(f\"Q max difference: {(triton_q_out - torch_q_out).abs().max().item()}\")\n",
    "        if not k_match:\n",
    "            print(f\"K max difference: {(triton_k_out - torch_k_out).abs().max().item()}\")\n",
    "\n",
    "# Run the test\n",
    "test_apply_rope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
