{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 64,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1695000,\n",
       " 'mem_clock_rate': 8001000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def apply_rope_kernel(\n",
    "    Q_ptr, K_ptr, cos_ptr, sin_ptr, pos_ids_ptr,\n",
    "    Q_out_ptr, K_out_ptr,\n",
    "    seq_len, num_heads, head_dim,\n",
    "    batch_stride, seq_stride, head_stride,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    DTYPE: tl.constexpr,\n",
    "):\n",
    "    # Identify which batch, sequence position, and head this program handles\n",
    "    pid_batch = tl.program_id(0)  # Batch index\n",
    "    pid_seq = tl.program_id(1)    # Sequence position\n",
    "    pid_head = tl.program_id(2)   # Head index\n",
    "    \n",
    "    # Get position ID for this sequence position\n",
    "    pos_id = tl.load(pos_ids_ptr + pid_batch * seq_len + pid_seq)\n",
    "    \n",
    "    # Calculate base pointer offset for Q and K\n",
    "    base_offset = (pid_batch * batch_stride + \n",
    "                   pid_seq * seq_stride + \n",
    "                   pid_head * head_stride)\n",
    "    \n",
    "    Q_start_ptr = Q_ptr + base_offset\n",
    "    K_start_ptr = K_ptr + base_offset\n",
    "    Q_out_start_ptr = Q_out_ptr + base_offset\n",
    "    K_out_start_ptr = K_out_ptr + base_offset\n",
    "    \n",
    "    # Process the head dimension in chunks\n",
    "    for k in range(0, tl.cdiv(head_dim, BLOCK_SIZE*2)):\n",
    "        # Calculate offsets for even elements (0, 2, 4, ...)\n",
    "        even_offsets = k * (BLOCK_SIZE*2) + tl.arange(0, BLOCK_SIZE) * 2\n",
    "        even_mask = even_offsets < head_dim\n",
    "        \n",
    "        # Calculate offsets for odd elements (1, 3, 5, ...)\n",
    "        odd_offsets = even_offsets + 1\n",
    "        odd_mask = odd_offsets < head_dim\n",
    "        \n",
    "        # Load even and odd elements separately\n",
    "        q_even = tl.load(Q_start_ptr + even_offsets, mask=even_mask)\n",
    "        q_odd = tl.load(Q_start_ptr + odd_offsets, mask=odd_mask)\n",
    "        k_even = tl.load(K_start_ptr + even_offsets, mask=even_mask)\n",
    "        k_odd = tl.load(K_start_ptr + odd_offsets, mask=odd_mask)\n",
    "        \n",
    "        # Load cos/sin values (one per pair)\n",
    "        cos_offsets = k * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "        cos_mask = cos_offsets < (head_dim // 2)\n",
    "        cos_vals = tl.load(cos_ptr + pos_id * (head_dim//2) + cos_offsets, mask=cos_mask)\n",
    "        sin_vals = tl.load(sin_ptr + pos_id * (head_dim//2) + cos_offsets, mask=cos_mask)\n",
    "        \n",
    "        # Apply rotation\n",
    "        q_even_out = q_even * cos_vals - q_odd * sin_vals\n",
    "        q_odd_out = q_odd * cos_vals + q_even * sin_vals\n",
    "        k_even_out = k_even * cos_vals - k_odd * sin_vals\n",
    "        k_odd_out = k_odd * cos_vals + k_even * sin_vals\n",
    "        \n",
    "        # Store results\n",
    "        tl.store(Q_out_start_ptr + even_offsets, q_even_out, mask=even_mask)\n",
    "        tl.store(Q_out_start_ptr + odd_offsets, q_odd_out, mask=odd_mask)\n",
    "        tl.store(K_out_start_ptr + even_offsets, k_even_out, mask=even_mask)\n",
    "        tl.store(K_out_start_ptr + odd_offsets, k_odd_out, mask=odd_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(q: torch.Tensor, k: torch.Tensor, pos_ids: torch.Tensor, cosines: torch.Tensor, sines: torch.Tensor):\n",
    "    q_out = torch.zeros_like(q)\n",
    "    k_out = torch.zeros_like(k)\n",
    "    B, L, H, D = q.shape\n",
    "    num_stages = 8\n",
    "    BLOCK_SIZE = 32\n",
    "    \n",
    "    # Grid dimensions should be (batch, sequence_length, num_heads)\n",
    "    grid = (B, L, H)\n",
    "    \n",
    "    apply_rope_kernel[grid](\n",
    "        q, k, cosines, sines, pos_ids,  # Input tensors\n",
    "        q_out, k_out,                   # Output tensors\n",
    "        L, H, D,                        # Dimensions\n",
    "        q.stride(0), q.stride(1), q.stride(2),  # Strides\n",
    "        BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages,\n",
    "        DTYPE=tl.float32\n",
    "    )\n",
    "    \n",
    "    return q_out, k_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch RoPE implementations match\n"
     ]
    }
   ],
   "source": [
    "# For comparing with native PyTorch implementation\n",
    "def torch_apply_rope(q, k, position_ids, head_dim):\n",
    "    # Create position embeddings\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=q.device).float() / head_dim))\n",
    "    \n",
    "    # Get sin and cos values\n",
    "    batch_size, seq_len = position_ids.shape\n",
    "    freqs = torch.einsum(\"b l, d -> b l d\", position_ids.float(), inv_freq)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = emb.cos()[:, :, None, :].repeat(1, 1, q.shape[2], 1)\n",
    "    sin = emb.sin()[:, :, None, :].repeat(1, 1, q.shape[2], 1)\n",
    "    \n",
    "    q_even = q[..., ::2]\n",
    "    q_odd = q[..., 1::2]\n",
    "    k_even = k[..., ::2]\n",
    "    k_odd = k[..., 1::2]\n",
    "\n",
    "    q_even_out = q_even * cos[..., :head_dim//2] - q_odd * sin[..., :head_dim//2]\n",
    "    q_odd_out = q_odd * cos[..., :head_dim//2] + q_even * sin[..., :head_dim//2]\n",
    "    k_even_out = k_even * cos[..., :head_dim//2] - k_odd * sin[..., :head_dim//2]\n",
    "    k_odd_out = k_odd * cos[..., :head_dim//2] + k_even * sin[..., :head_dim//2]\n",
    "\n",
    "    # Recombine\n",
    "    q_out = torch.zeros_like(q)\n",
    "    k_out = torch.zeros_like(k)\n",
    "    q_out[..., ::2] = q_even_out\n",
    "    q_out[..., 1::2] = q_odd_out\n",
    "    k_out[..., ::2] = k_even_out\n",
    "    k_out[..., 1::2] = k_odd_out\n",
    "    \n",
    "    return q_out, k_out\n",
    "\n",
    "# Test function\n",
    "def test_apply_rope():\n",
    "    # Set up test data\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    num_heads = 4\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Generate test data\n",
    "    q = torch.randn((batch_size, seq_len, num_heads, head_dim), device=device, dtype=torch.float32)\n",
    "    k = torch.randn((batch_size, seq_len, num_heads, head_dim), device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Position IDs\n",
    "    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "    \n",
    "    # Precompute cosines and sines\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))\n",
    "    \n",
    "    # Initialize cosine and sine tables\n",
    "    cos_table = torch.zeros((seq_len, head_dim // 2), device=device)\n",
    "    sin_table = torch.zeros((seq_len, head_dim // 2), device=device)\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        freqs = pos * inv_freq\n",
    "        cos_table[pos] = freqs.cos()\n",
    "        sin_table[pos] = freqs.sin()\n",
    "    \n",
    "    # Run your Triton version\n",
    "    triton_q_out, triton_k_out = apply_rope(q, k, position_ids, cos_table, sin_table)\n",
    "    \n",
    "    # Run PyTorch reference\n",
    "    torch_q_out, torch_k_out = torch_apply_rope(q, k, position_ids, head_dim)\n",
    "    \n",
    "    # Compare results\n",
    "    q_match = torch.allclose(triton_q_out, torch_q_out, atol=1e-5, rtol=1e-5)\n",
    "    k_match = torch.allclose(triton_k_out, torch_k_out, atol=1e-5, rtol=1e-5)\n",
    "    \n",
    "    if q_match and k_match:\n",
    "        print(\"✅ Triton and Torch RoPE implementations match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch RoPE implementations differ\")\n",
    "        if not q_match:\n",
    "            print(f\"Q max difference: {(triton_q_out - torch_q_out).abs().max().item()}\")\n",
    "        if not k_match:\n",
    "            print(f\"K max difference: {(triton_k_out - torch_k_out).abs().max().item()}\")\n",
    "\n",
    "# Run the test\n",
    "test_apply_rope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
